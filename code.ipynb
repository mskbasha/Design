{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 12:41:22.818176: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-09 12:41:25.298305: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-09 12:41:25.298704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-09 12:41:26.239269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-09 12:41:26.988949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-09 12:41:33.141986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"cat.jpg\")\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_processor import VideoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frame chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10908/10908 [04:24<00:00, 41.28it/s]\n",
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:139: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, _ = librosa.load(video_path, sr=16000)\n",
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 211742.52it/s]\n"
     ]
    }
   ],
   "source": [
    "vp = VideoProcessor(\n",
    "    clip_model.text_model,\n",
    "    model,\n",
    "    processor.tokenizer,\n",
    "    image_processor,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "video,audio = vp.extract_frames_and_audio(\n",
    "    \"/DATA/sarmistha_2221cs21/basha/VideoMAE/dataset_224x224/_gPgueccVzw.mp4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"frames_audio__gPgueccVzw.pkl\",'wb') as f:\n",
    "    pickle.dump([video,audio],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  return torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "encoding = vp.vision_encoder(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "video_dir = '/home/basha_2211ai03/complaint_detection/videos/dataset'\n",
    "videos = os.listdir(video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   932,  3337, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('basha',return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "try:\n",
    "    with open(\"data.pkl\",'rb') as f:\n",
    "        encoded_data = pickle.load(f)\n",
    "except:\n",
    "    encoded_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497753d66efc44389cc217b595dd1b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:208\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/home/basha_2211ai03/complaint_detection/videos/dataset/R6RHwjnibA0.mp4': Format not recognised.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m encoded_data:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m encoded_data[video] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/complaint_detection/Design/video_processor.py:55\u001b[0m, in \u001b[0;36mVideoProcessor.__call__\u001b[0;34m(self, video_loc)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_loc: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor:\n\u001b[0;32m---> 55\u001b[0m     frames, audios \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_frames_and_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_loc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     encoded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_encoder(frames)\n\u001b[1;32m     57\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_encoder(audios)\n",
      "File \u001b[0;32m~/complaint_detection/Design/video_processor.py:109\u001b[0m, in \u001b[0;36mVideoProcessor.extract_frames_and_audio\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m frame_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS))\n\u001b[1;32m    108\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT))\n\u001b[0;32m--> 109\u001b[0m audio_data, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    112\u001b[0m audio_clips \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:183\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/util/decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:239\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    242\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/audioread/__init__.py:132\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
      "\u001b[0;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "with torch.inference_mode():\n",
    "    for video in tqdm(videos):\n",
    "        if video in encoded_data:\n",
    "            continue\n",
    "        out = vp(os.path.join(video_dir,video))\n",
    "        encoded_data[video] = out\n",
    "        with open(\"data.pkl\",'wb') as f:\n",
    "            pickle.dump(encoded_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data.pkl\",'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = vp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   932,  3337, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('basha',return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.mean(1).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data.pkl\",'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Lb0mjQ50hkg.mp4'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DetectSpan\n",
    "detect_span_model = DetectSpan(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_span_model  = detect_span_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Lb0mjQ50hkg.mp4'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def pad(batch: List[torch.tensor], dim: int = 512, max_len: int = None) -> torch.tensor:\n",
    "    if max_len is None:\n",
    "        max_len = max([len(x) for x in batch])\n",
    "    attention_mask = []\n",
    "    for i in range(len(batch)):\n",
    "        ones = [1] * len(batch[i])\n",
    "        pad_length = max_len - len(batch[i])\n",
    "        if pad_length == 0:\n",
    "            attention_mask.append(ones)\n",
    "            continue\n",
    "        zeros = [0] * pad_length\n",
    "        padding = torch.zeros(pad_length, dim)\n",
    "        batch[i] = torch.cat([batch[i], padding ])\n",
    "        attention_mask.append(ones + zeros)\n",
    "    return torch.stack(batch), torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   932,  3337, 49407],\n",
       "        [49406,   328, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer(['basha','i'],return_tensors='pt',padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Lb0mjQ50hkg.mp4'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([83, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['VBebt51tOUE.mp4'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pad([dataset['Lb0mjQ50hkg.mp4'][0].cpu(),dataset['VBebt51tOUE.mp4'][0].cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "\n",
    "class DetectSpan(nn.Module):\n",
    "    \"\"\"Model to detect span in a video\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): input shape to the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, nhead=12, d_model=768,num_blocks = 12):\n",
    "        \"\"\"Init method for DetectSpan.\"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True),\n",
    "            num_layers=num_blocks,\n",
    "        )\n",
    "        self.project = nn.Linear(input_shape,d_model)\n",
    "        self.classification_layer = nn.Linear(d_model, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, attention_mask, labels=None):\n",
    "        x = self.project(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=attention_mask.to(torch.bool))\n",
    "        x = self.classification_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        if labels:\n",
    "            return self.loss(x, labels)\n",
    "        return x\n",
    "\n",
    "    def loss(self, model_outputs, labels):\n",
    "        self.loss_function(model_outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_span_model = DetectSpan(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_span_model = detect_span_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetectSpan(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (project): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (classification_layer): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (loss_function): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_span_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1375,  0.0839,  0.0598,  ..., -0.1668, -0.1738,  0.1406],\n",
      "         [ 0.1133,  0.1077,  0.0212,  ..., -0.2659, -0.1425,  0.0915],\n",
      "         [ 0.1456,  0.1448, -0.0016,  ..., -0.1683, -0.1475,  0.1722],\n",
      "         ...,\n",
      "         [ 0.0587, -0.1146,  0.0255,  ..., -0.1927, -0.1174,  0.1000],\n",
      "         [ 0.0732,  0.0676, -0.0312,  ..., -0.0893, -0.5749,  0.0433],\n",
      "         [ 0.0810, -0.0746, -0.0896,  ..., -0.1709, -0.5200, -0.0180]],\n",
      "\n",
      "        [[-0.2935,  0.3012, -0.0762,  ...,  0.0495, -0.2012, -0.0411],\n",
      "         [-0.2836,  0.2760, -0.1742,  ..., -0.0578, -0.2003,  0.0056],\n",
      "         [-0.1369,  0.1105, -0.4463,  ..., -0.1642, -0.1567,  0.1483],\n",
      "         ...,\n",
      "         [ 0.0253,  0.0211,  0.0414,  ...,  0.0421,  0.0236,  0.0368],\n",
      "         [ 0.0253,  0.0211,  0.0414,  ...,  0.0421,  0.0236,  0.0368],\n",
      "         [ 0.0253,  0.0211,  0.0414,  ...,  0.0421,  0.0236,  0.0368]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
      "\n",
      "        [[ 0.0466, -0.3322, -0.5900,  ...,  0.0691,  0.0175,  1.0977],\n",
      "         [-0.3704, -0.2617, -1.0337,  ...,  0.2113,  0.3018,  1.2070],\n",
      "         [-0.0341, -0.3697, -1.2946,  ...,  0.3555, -0.0173,  0.9859],\n",
      "         ...,\n",
      "         [-0.3127, -0.1508, -1.1490,  ...,  0.5151,  0.2730,  1.0339],\n",
      "         [-0.5548, -0.3771, -1.0495,  ...,  0.2438,  0.0305,  1.1428],\n",
      "         [-0.3353, -0.2860, -0.9808,  ...,  0.4727, -0.1409,  1.1178]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan],\n",
      "         ...,\n",
      "         [   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan]],\n",
      "\n",
      "        [[0.4180, 0.9904, 0.5982],\n",
      "         [0.5622, 0.8876, 0.7336],\n",
      "         [0.5576, 0.5879, 0.6636],\n",
      "         ...,\n",
      "         [0.6553, 0.5693, 0.5844],\n",
      "         [0.5304, 0.8418, 0.6009],\n",
      "         [0.5807, 0.6960, 0.6955]]], device='cuda:0')\n",
      "tensor([[[   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan],\n",
      "         ...,\n",
      "         [   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan]],\n",
      "\n",
      "        [[0.6030, 0.7292, 0.6452],\n",
      "         [0.6370, 0.7084, 0.6756],\n",
      "         [0.6359, 0.6429, 0.6601],\n",
      "         ...,\n",
      "         [0.6582, 0.6386, 0.6421],\n",
      "         [0.6296, 0.6988, 0.6459],\n",
      "         [0.6412, 0.6673, 0.6672]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    masked = detect_span_model(out[0].cuda(), attention_mask=out[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6030, 0.7292, 0.6452],\n",
       "        [0.6370, 0.7084, 0.6756],\n",
       "        [0.6359, 0.6429, 0.6601],\n",
       "        [0.6455, 0.6558, 0.6613],\n",
       "        [0.5199, 0.6695, 0.6153],\n",
       "        [0.6228, 0.6745, 0.6378],\n",
       "        [0.6518, 0.6133, 0.6677],\n",
       "        [0.6133, 0.6674, 0.6346],\n",
       "        [0.6342, 0.6678, 0.6494],\n",
       "        [0.6464, 0.6610, 0.6514],\n",
       "        [0.6231, 0.6689, 0.6556],\n",
       "        [0.5927, 0.6622, 0.6374],\n",
       "        [0.5849, 0.6476, 0.6199],\n",
       "        [0.6048, 0.6513, 0.6388],\n",
       "        [0.6283, 0.6886, 0.6677],\n",
       "        [0.6119, 0.6781, 0.6486],\n",
       "        [0.6021, 0.6478, 0.6356],\n",
       "        [0.6602, 0.6594, 0.6226],\n",
       "        [0.5874, 0.6849, 0.6599],\n",
       "        [0.6031, 0.6417, 0.6729],\n",
       "        [0.6053, 0.6799, 0.6258],\n",
       "        [0.6020, 0.7039, 0.6235],\n",
       "        [0.5559, 0.6898, 0.6699],\n",
       "        [0.6259, 0.6686, 0.6573],\n",
       "        [0.6373, 0.6943, 0.6354],\n",
       "        [0.5863, 0.6953, 0.6746],\n",
       "        [0.6482, 0.7212, 0.6743],\n",
       "        [0.5855, 0.6682, 0.7138],\n",
       "        [0.5938, 0.6614, 0.6784],\n",
       "        [0.6092, 0.6723, 0.6824],\n",
       "        [0.6402, 0.6431, 0.6137],\n",
       "        [0.6046, 0.6777, 0.6602],\n",
       "        [0.6364, 0.7010, 0.6612],\n",
       "        [0.6385, 0.7248, 0.6224],\n",
       "        [0.6616, 0.6839, 0.6940],\n",
       "        [0.5854, 0.6791, 0.6786],\n",
       "        [0.5565, 0.6728, 0.6392],\n",
       "        [0.5717, 0.7250, 0.6205],\n",
       "        [0.5761, 0.7305, 0.6340],\n",
       "        [0.6453, 0.6845, 0.6563],\n",
       "        [0.6226, 0.6804, 0.6531],\n",
       "        [0.6020, 0.6703, 0.6264],\n",
       "        [0.6059, 0.6573, 0.6409],\n",
       "        [0.5971, 0.6667, 0.6696],\n",
       "        [0.6246, 0.6822, 0.6517],\n",
       "        [0.6280, 0.6974, 0.6381],\n",
       "        [0.5908, 0.7122, 0.6399],\n",
       "        [0.6540, 0.7079, 0.6188],\n",
       "        [0.6072, 0.6943, 0.6585],\n",
       "        [0.5599, 0.6828, 0.6466],\n",
       "        [0.5596, 0.6936, 0.6334],\n",
       "        [0.6140, 0.6517, 0.5835],\n",
       "        [0.5997, 0.6721, 0.6523],\n",
       "        [0.6075, 0.6724, 0.6952],\n",
       "        [0.6050, 0.7076, 0.5775],\n",
       "        [0.6163, 0.6867, 0.6040],\n",
       "        [0.6231, 0.6502, 0.6364],\n",
       "        [0.6628, 0.7113, 0.6636],\n",
       "        [0.6270, 0.7088, 0.6693],\n",
       "        [0.6137, 0.6884, 0.6766],\n",
       "        [0.6276, 0.6644, 0.6629],\n",
       "        [0.6739, 0.7017, 0.6609],\n",
       "        [0.5970, 0.6721, 0.6901],\n",
       "        [0.6356, 0.6947, 0.5932],\n",
       "        [0.6135, 0.6980, 0.6480],\n",
       "        [0.5807, 0.6592, 0.6589],\n",
       "        [0.5518, 0.7331, 0.6717],\n",
       "        [0.6610, 0.7251, 0.6187],\n",
       "        [0.5966, 0.7146, 0.7078],\n",
       "        [0.6055, 0.6352, 0.6357],\n",
       "        [0.6042, 0.6677, 0.6694],\n",
       "        [0.6435, 0.6846, 0.6577],\n",
       "        [0.6197, 0.6760, 0.6493],\n",
       "        [0.6569, 0.6681, 0.6642],\n",
       "        [0.6145, 0.7085, 0.6621],\n",
       "        [0.6084, 0.6864, 0.6560],\n",
       "        [0.5765, 0.6811, 0.6208],\n",
       "        [0.5910, 0.6818, 0.6209],\n",
       "        [0.5952, 0.6885, 0.6690],\n",
       "        [0.6028, 0.7051, 0.6584],\n",
       "        [0.6507, 0.7036, 0.6793],\n",
       "        [0.6492, 0.6474, 0.6482],\n",
       "        [0.5955, 0.6990, 0.6632],\n",
       "        [0.6102, 0.6767, 0.6544],\n",
       "        [0.6336, 0.6962, 0.7384],\n",
       "        [0.6600, 0.6868, 0.6799],\n",
       "        [0.6873, 0.6636, 0.6876],\n",
       "        [0.6585, 0.6506, 0.6291],\n",
       "        [0.6466, 0.6505, 0.5845],\n",
       "        [0.6273, 0.6960, 0.6434],\n",
       "        [0.6367, 0.6370, 0.6669],\n",
       "        [0.6402, 0.6394, 0.7289],\n",
       "        [0.6290, 0.6747, 0.6933],\n",
       "        [0.6340, 0.6672, 0.5960],\n",
       "        [0.6181, 0.6773, 0.6623],\n",
       "        [0.6287, 0.6231, 0.6620],\n",
       "        [0.5960, 0.7077, 0.6614],\n",
       "        [0.6450, 0.6278, 0.6683],\n",
       "        [0.6230, 0.6370, 0.6435],\n",
       "        [0.6237, 0.6592, 0.6256],\n",
       "        [0.6478, 0.6955, 0.5894],\n",
       "        [0.6105, 0.6403, 0.6462],\n",
       "        [0.5833, 0.6733, 0.6334],\n",
       "        [0.5822, 0.7251, 0.6864],\n",
       "        [0.6390, 0.6527, 0.6867],\n",
       "        [0.5890, 0.6865, 0.6688],\n",
       "        [0.6428, 0.6541, 0.6467],\n",
       "        [0.6408, 0.6493, 0.6534],\n",
       "        [0.5962, 0.6602, 0.6503],\n",
       "        [0.6182, 0.6805, 0.6206],\n",
       "        [0.6627, 0.6285, 0.7026],\n",
       "        [0.6278, 0.6734, 0.6352],\n",
       "        [0.6393, 0.6586, 0.6584],\n",
       "        [0.5879, 0.6893, 0.6281],\n",
       "        [0.6344, 0.6636, 0.6538],\n",
       "        [0.6164, 0.6747, 0.6134],\n",
       "        [0.5647, 0.6530, 0.6477],\n",
       "        [0.6155, 0.6827, 0.6902],\n",
       "        [0.6275, 0.6877, 0.6662],\n",
       "        [0.6596, 0.6763, 0.6843],\n",
       "        [0.6443, 0.6557, 0.6278],\n",
       "        [0.6233, 0.6770, 0.6414],\n",
       "        [0.6572, 0.6106, 0.6241],\n",
       "        [0.6489, 0.6436, 0.6359],\n",
       "        [0.5918, 0.6740, 0.5602],\n",
       "        [0.6621, 0.6286, 0.6677],\n",
       "        [0.6260, 0.6682, 0.6894],\n",
       "        [0.6503, 0.6710, 0.6057],\n",
       "        [0.5458, 0.6520, 0.6403],\n",
       "        [0.6038, 0.6053, 0.6538],\n",
       "        [0.6336, 0.6714, 0.6549],\n",
       "        [0.6363, 0.6740, 0.6514],\n",
       "        [0.6405, 0.6779, 0.6549],\n",
       "        [0.6428, 0.6689, 0.6333],\n",
       "        [0.6312, 0.6503, 0.6589],\n",
       "        [0.6037, 0.6449, 0.6411],\n",
       "        [0.6076, 0.7139, 0.6785],\n",
       "        [0.6752, 0.7248, 0.6770],\n",
       "        [0.5859, 0.6639, 0.6606],\n",
       "        [0.6285, 0.6634, 0.6217],\n",
       "        [0.5906, 0.7060, 0.6347],\n",
       "        [0.6370, 0.6915, 0.6330],\n",
       "        [0.6078, 0.6278, 0.6691],\n",
       "        [0.6624, 0.6922, 0.7002],\n",
       "        [0.6370, 0.6793, 0.6410],\n",
       "        [0.6066, 0.6851, 0.6124],\n",
       "        [0.6446, 0.6894, 0.6640],\n",
       "        [0.5857, 0.6563, 0.6264],\n",
       "        [0.6347, 0.6639, 0.6327],\n",
       "        [0.5597, 0.6835, 0.6528],\n",
       "        [0.6527, 0.6717, 0.6462],\n",
       "        [0.5826, 0.6764, 0.6458],\n",
       "        [0.6635, 0.6893, 0.6510],\n",
       "        [0.6358, 0.6429, 0.6889],\n",
       "        [0.6695, 0.6590, 0.6046],\n",
       "        [0.6558, 0.6873, 0.7025],\n",
       "        [0.6631, 0.6214, 0.6804],\n",
       "        [0.6393, 0.6719, 0.6705],\n",
       "        [0.6730, 0.6181, 0.6736],\n",
       "        [0.6427, 0.6398, 0.5980],\n",
       "        [0.6266, 0.6566, 0.6292],\n",
       "        [0.6477, 0.6657, 0.6575],\n",
       "        [0.6709, 0.6680, 0.6287],\n",
       "        [0.6473, 0.6828, 0.6475],\n",
       "        [0.6372, 0.7019, 0.6306],\n",
       "        [0.6416, 0.6893, 0.6557],\n",
       "        [0.6315, 0.6758, 0.6123],\n",
       "        [0.6904, 0.6592, 0.6954],\n",
       "        [0.6008, 0.6761, 0.6794],\n",
       "        [0.6472, 0.7297, 0.6666],\n",
       "        [0.6972, 0.6883, 0.6177],\n",
       "        [0.6555, 0.6509, 0.6196],\n",
       "        [0.6482, 0.6491, 0.5919],\n",
       "        [0.6614, 0.6898, 0.6576],\n",
       "        [0.6789, 0.6228, 0.6638],\n",
       "        [0.6005, 0.6599, 0.6837],\n",
       "        [0.6536, 0.7009, 0.6804],\n",
       "        [0.6703, 0.6754, 0.6610],\n",
       "        [0.6071, 0.6694, 0.6730],\n",
       "        [0.6815, 0.6574, 0.6311],\n",
       "        [0.6487, 0.6396, 0.6999],\n",
       "        [0.6406, 0.6740, 0.6293],\n",
       "        [0.6573, 0.6552, 0.6108],\n",
       "        [0.6561, 0.6649, 0.5511],\n",
       "        [0.6964, 0.6293, 0.6856],\n",
       "        [0.6192, 0.6673, 0.6233],\n",
       "        [0.6346, 0.6453, 0.6886],\n",
       "        [0.6941, 0.6926, 0.6804],\n",
       "        [0.6296, 0.6857, 0.6442],\n",
       "        [0.5982, 0.6819, 0.6643],\n",
       "        [0.6456, 0.6334, 0.6514],\n",
       "        [0.6582, 0.6386, 0.6421],\n",
       "        [0.6296, 0.6988, 0.6459],\n",
       "        [0.6412, 0.6673, 0.6672]], device='cuda:0')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "encoder = BertModel(BertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = processor.tokenizer(['basha','sha'],return_tensors = 'pt',padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406, 11420, 49407]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('bas',return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = clip_model.text_model(\n",
    "    input_ids = inputs['input_ids'],\n",
    "    attention_mask = inputs['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.last_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
