{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 09:39:43.630294: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-04 09:39:43.700875: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 09:39:43.701004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 09:39:43.704502: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 09:39:43.720230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 09:39:46.732945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"cat.jpg\")\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_processor import VideoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VideoProcessor(\n",
    "  clip_model.text_model,\n",
    "  clip_model.vision_model,\n",
    "  processor.tokenizer,\n",
    "  processor.image_processor,\n",
    "  clip_model.text_projection,\n",
    "  clip_model.visual_projection,\n",
    "  device = 'cpu'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = vp.image_processor(\n",
    "    [image,image]\n",
    ")['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.tensor(\n",
    "    images\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:109: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, _ = librosa.load(video_path, sr=16000)\n",
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2 sec batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [02:35<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2 sec batches complete\n"
     ]
    }
   ],
   "source": [
    "out = vp.extract_frames_and_audio(\"/DATA/sarmistha_2221cs21/basha/VideoMAE/dataset/_A5qfpLTbns.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data.pkl\",'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = vp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoProcessor(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (audio_model): Whisper(\n",
       "    (encoder): AudioEncoder(\n",
       "      (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x ResidualAttentionBlock(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TextDecoder(\n",
       "      (token_embedding): Embedding(51865, 512)\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x ResidualAttentionBlock(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn): MultiHeadAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (image_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:109: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, _ = librosa.load(video_path, sr=16000)\n",
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2 sec batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [02:24<00:00,  1.25it/s]\n",
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:143: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  converted_clips = torch.tensor(frames)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2 sec batches complete\n",
      "Encoding frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:41<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding frames done\n",
      "Extracting text from audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180 [00:00<?, ?it/s]/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/whisper/transcribe.py:113: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 180/180 [05:49<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from audio completed\n",
      "Encoding Text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:10<00:00, 17.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vision = vp(\"/DATA/sarmistha_2221cs21/basha/VideoMAE/dataset/_A5qfpLTbns.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2062e-01,  4.5014e-02, -1.6820e-01, -3.4746e-02, -2.1113e-01,\n",
       "         -1.3153e-01,  1.2401e-01, -5.0429e-01, -1.8531e-02, -2.3232e-01,\n",
       "         -1.8971e-02, -3.7197e-02,  3.0465e-01,  4.0085e-01,  1.2778e-01,\n",
       "          1.1729e-01, -9.8357e-02, -9.7343e-02,  3.2400e-01, -3.3887e-01,\n",
       "          9.2911e-02, -2.6583e-01, -2.2583e-02, -7.6680e-02, -1.9461e-01,\n",
       "         -2.8250e-01,  1.1555e-01,  4.3266e-02, -5.6809e-02, -3.5991e-02,\n",
       "          4.3223e-02,  1.4839e-01,  1.7368e-01, -1.0038e-01, -1.2956e-01,\n",
       "          7.6926e-02, -2.2662e-01, -1.6489e-01,  1.6700e-01,  7.5899e-02,\n",
       "         -2.9313e-01, -2.8349e-01,  1.0109e-01,  8.1005e-03,  1.0431e-01,\n",
       "         -8.2173e-04,  1.4887e-01, -5.7052e-02, -7.0436e-03, -1.3565e-01,\n",
       "         -1.6587e-01,  3.3023e-01, -9.7329e-02,  1.3642e-01,  1.0315e-01,\n",
       "         -3.0204e-01, -1.8882e-02, -1.0632e-01,  3.2190e-01, -1.3384e-03,\n",
       "         -4.9162e-01, -3.6689e-01,  7.5371e-02,  4.1403e-03, -5.3402e-02,\n",
       "          9.5969e-02, -1.8406e-02,  4.9326e-01, -1.1642e-01, -7.6122e-02,\n",
       "         -1.4696e-01,  1.3152e-01, -4.2859e-01, -1.4945e-01, -2.2199e-01,\n",
       "         -2.4869e-01,  3.2821e-01,  1.0712e-01,  6.2700e-02, -1.0571e-01,\n",
       "          2.1466e-01, -9.5781e-02,  2.1642e-01,  4.2144e-01, -1.4273e-01,\n",
       "         -9.4595e-02,  1.4406e-01,  2.1464e-01,  4.7440e-01,  1.7754e-01,\n",
       "         -2.6204e-02,  5.8990e-02, -1.5311e+00, -5.5436e-02, -2.3580e-02,\n",
       "         -9.1653e-02, -2.2639e-01, -7.3488e-02, -4.2806e-01, -1.3058e-01,\n",
       "         -6.0936e-02, -1.2356e-01,  1.0361e-01,  1.8016e-01, -6.4200e-01,\n",
       "          2.0616e-01, -1.1322e-01, -1.5625e-01,  3.3891e-01,  3.2797e-02,\n",
       "         -3.4901e-01,  1.2080e+00, -1.8000e-01, -4.6432e-02, -1.8502e-01,\n",
       "          1.8249e-01,  3.4240e-02,  3.4221e-01, -6.4567e-02,  2.0944e-01,\n",
       "          3.6807e-02, -5.6738e-01,  9.9366e-02,  8.1468e-02,  1.7766e-01,\n",
       "          5.5506e-02,  2.9099e-02, -4.4288e-01, -2.7577e-02, -1.5497e-02,\n",
       "          2.8310e-01, -2.7419e-01,  1.7272e-01,  6.9542e+00, -5.9301e-02,\n",
       "         -1.9976e-01, -5.7875e-02,  3.6375e-01, -1.9124e-01, -1.3699e-01,\n",
       "         -2.7796e-01,  2.8606e-01,  9.1733e-02, -1.3766e-01,  8.4975e-02,\n",
       "         -1.2306e-01, -5.9397e-02, -1.7774e-01,  1.7979e-01, -5.9592e-01,\n",
       "          1.0019e-01, -6.0563e-02,  1.4635e-01,  1.4679e-01, -1.4380e-01,\n",
       "         -7.2905e-02,  7.4343e-02,  7.2663e-02, -5.0178e-02, -6.4885e-02,\n",
       "         -3.3593e-01,  2.0282e-01,  4.3937e-02, -2.8235e-03,  2.0563e-01,\n",
       "          4.9436e-02,  1.8013e-01, -4.0452e-01, -1.1557e-01,  8.1550e-02,\n",
       "         -2.4822e-01, -2.7796e-02, -1.1839e-01, -4.4283e-01,  3.7714e-01,\n",
       "          6.7154e-02, -4.1783e-01,  3.4957e-01,  1.6731e-01, -6.9683e-02,\n",
       "          1.0747e-01, -1.2307e-01,  9.0107e-04, -3.8167e-02,  2.0420e-01,\n",
       "          1.5939e-01, -3.9695e-02,  2.7748e-01, -2.5392e-01, -1.6328e-01,\n",
       "          3.1911e-01, -1.3006e-01, -2.2918e-01, -1.8566e-01,  1.4489e-01,\n",
       "          1.1283e-01, -2.2713e-01,  2.0417e-01,  2.0243e-01, -6.0990e-02,\n",
       "          2.7731e-02, -2.5567e-01,  1.5764e-01,  5.8029e-02,  7.6540e-02,\n",
       "          1.7570e-02, -2.7825e-02,  1.1061e-01, -2.9641e-02, -3.6532e-01,\n",
       "         -3.2033e-01, -1.0032e-01,  7.4519e-02,  3.3865e-01, -2.2703e-02,\n",
       "          1.0843e-01, -1.1913e-01,  9.6855e-02, -9.5795e-01,  3.2880e-02,\n",
       "         -1.6678e-01, -3.7853e-01, -1.6139e-01, -7.3266e-02, -2.2800e-01,\n",
       "          2.0200e-02, -1.6915e-01,  1.2691e-01,  2.7316e-01, -7.0726e-02,\n",
       "         -8.6809e-02, -6.2879e-02,  1.4526e-01, -7.1114e-02,  2.2127e-01,\n",
       "         -7.4600e-01,  1.6527e-01,  5.0748e-02, -1.2086e-01,  5.3782e-02,\n",
       "         -1.0509e-01, -1.5563e-01, -3.4978e-01,  4.6274e-02, -1.5276e-01,\n",
       "         -4.0213e-01,  9.5778e-02,  1.9698e-01,  5.1653e-02,  2.5275e-01,\n",
       "         -2.7577e-01, -1.9201e-01,  6.0601e-02,  9.5690e-02,  9.7773e-03,\n",
       "          4.7490e-02,  5.2865e-02, -3.8299e-03, -2.8724e-01, -1.4282e-01,\n",
       "         -3.6076e-01,  1.4987e-01,  6.4112e-02, -2.5446e-01, -6.5951e-02,\n",
       "         -7.2912e-02,  2.3107e-01,  1.4617e-01,  1.9920e-01, -6.0857e-02,\n",
       "          2.6100e-03, -2.2665e-01,  1.5437e-02, -8.6318e-02,  1.4716e-03,\n",
       "         -2.2034e-01, -1.5056e-01,  2.2503e-02, -1.8632e-01, -2.4931e-01,\n",
       "         -3.4723e-01,  2.8887e-01,  7.9560e-03, -1.4455e-03,  5.8414e-02,\n",
       "          4.5156e-01, -8.5148e-02, -1.9958e-01, -4.8804e-01,  3.2247e-02,\n",
       "          4.3469e-01, -4.3739e-02,  2.1662e-02,  1.0620e-01, -5.3439e-01,\n",
       "         -1.3055e-01, -2.2708e-01,  8.6595e-02, -3.7433e-02, -1.6968e-02,\n",
       "          7.8762e-02,  3.3803e-02,  1.2651e-01,  2.6457e-01,  9.2976e-02,\n",
       "          5.9110e-01,  1.4693e-01, -1.8926e-01,  5.4706e-02,  8.8293e-02,\n",
       "         -3.8535e-02, -1.9333e-01,  6.9510e+00,  5.0436e-02,  5.6331e-03,\n",
       "          9.2979e-02,  3.2365e-02,  4.8644e-01,  2.0073e-01,  1.2063e-01,\n",
       "          7.9998e-02,  2.6147e-01,  9.3003e-02, -5.2280e-02,  3.4084e-01,\n",
       "         -7.8207e-02,  4.7570e-02,  3.6292e-01,  1.1632e-01, -2.1962e+00,\n",
       "          2.5538e-01,  4.6117e-01,  2.8377e-01,  5.2169e-02, -1.4935e-02,\n",
       "          1.5080e-01,  1.3594e-01,  1.6760e-01,  3.0506e-01,  9.8981e-02,\n",
       "         -1.2392e-01, -6.3797e-02, -1.8362e-01,  1.5850e-01, -8.1656e-02,\n",
       "          6.2340e-02,  2.5344e-02,  3.0398e-01, -1.3185e-01,  3.2100e-02,\n",
       "          3.2883e-02,  3.0472e-01,  1.4305e-01,  3.1772e-01,  3.2137e-01,\n",
       "          6.0958e-01, -2.0709e-02,  9.2511e-02,  1.2298e-01, -5.1150e-01,\n",
       "         -3.7681e-01, -1.0083e-01,  1.1244e-01, -1.6162e-01, -2.3407e-01,\n",
       "         -8.3858e-02, -4.2550e-02,  4.2596e-01,  1.0608e-01, -7.3128e-02,\n",
       "          4.2857e-01,  4.4930e-01, -2.3494e-01, -1.7979e-01,  1.4967e-01,\n",
       "         -3.8301e-02, -2.2841e-01, -3.0592e-01, -1.9007e-01,  1.0432e-01,\n",
       "          4.2148e-01,  3.2286e-01, -2.9304e-01, -2.1553e-01, -1.4248e-01,\n",
       "          1.8091e-01,  9.7168e-02,  6.1880e-02,  5.7987e-02, -6.4734e-01,\n",
       "         -2.7251e-01,  2.5392e-01,  2.1647e-01, -2.3850e-01,  2.7617e-01,\n",
       "          3.5644e-01, -1.4517e-01,  3.2827e-02, -6.3215e-02,  3.8910e-01,\n",
       "          4.9750e-02, -4.1646e-01, -3.7541e-01,  3.6366e-01, -3.1982e-01,\n",
       "         -8.4147e-02,  2.1066e-01,  5.3055e-01,  2.7512e-01,  4.5504e-02,\n",
       "         -1.3629e-01,  1.7872e-01,  2.5893e-01,  1.8962e-02, -5.2401e-02,\n",
       "          1.8570e-01,  2.4235e-01,  2.5826e-02,  4.3108e-01,  3.8809e-01,\n",
       "         -1.1260e-01, -2.0355e-01, -3.9983e-02, -1.2299e-01, -2.9230e-01,\n",
       "         -8.8845e-03, -7.7554e-02,  4.6305e-01, -1.4372e-01, -1.6988e-01,\n",
       "          2.4974e-01,  2.2300e-01,  1.8829e-03,  9.0630e-02, -1.0492e-01,\n",
       "         -3.0206e-01,  5.3917e-02,  2.0903e-01, -2.2727e-01, -4.1900e-01,\n",
       "         -2.5885e-01,  6.0922e-02, -9.1705e-02,  3.2239e-01, -1.4309e-01,\n",
       "          1.0093e-01,  6.2606e-02, -2.3036e-01,  1.3927e-01, -6.4014e-02,\n",
       "          2.9734e-01,  1.1905e-01,  8.6867e-03,  8.5122e-02,  8.4519e-03,\n",
       "         -1.2967e-01,  3.2329e-01, -1.8781e-01,  2.1230e-01,  4.8638e-01,\n",
       "          1.7768e-01,  1.5441e-01, -2.7755e-01,  6.4519e-02, -1.9959e-01,\n",
       "          2.1285e-01, -2.4689e-01,  5.5606e-02, -1.6996e-01, -9.8415e-02,\n",
       "          9.9920e-02, -7.4238e-02, -2.0113e-02,  1.3819e-01, -4.0868e-02,\n",
       "          4.5768e-01, -2.3049e-02,  6.3881e-02,  1.5413e-01,  1.5993e-01,\n",
       "          3.6332e-01, -1.5200e-01, -5.3066e-02, -1.8015e-01,  1.2580e-02,\n",
       "         -1.8310e-01, -1.2623e-01, -2.5761e-02, -7.3246e-02, -4.2941e-02,\n",
       "          1.1606e-01, -1.1271e-01,  7.2115e-02, -7.0163e-01, -4.4164e-02,\n",
       "          1.5147e-01,  1.9115e-01,  3.4653e-01, -9.2961e-02, -9.8659e-03,\n",
       "          4.0687e-01,  2.7851e-01, -2.2720e-01, -3.1043e-02,  1.2847e-01,\n",
       "         -1.2166e-01, -2.1689e-02,  1.1084e-01, -1.4049e-01,  1.6401e-01,\n",
       "         -2.5425e-01,  2.5947e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model.text_projection(vision[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vision[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vp.image_model(**processor.image_processor([image,image], return_tensors=\"pt\").to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.pooler_output.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer('basha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = clip_model.text_model(\n",
    "    input_ids = inputs['input_ids'],\n",
    "    attention_mask = inputs['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.last_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
