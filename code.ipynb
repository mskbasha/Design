{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import librosa\n",
    "import whisper\n",
    "import os\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"cat.jpg\")\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_processor import VideoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VideoProcessor(\n",
    "  clip_model.text_model.cuda(),\n",
    "  clip_model.vision_model.cuda(),\n",
    "  processor.tokenizer,\n",
    "  processor.image_processor  \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = vp.image_processor(\n",
    "    [image,image]\n",
    ")['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1712591/1127037752.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch.tensor(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(\n",
    "    images\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:97: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, _ = librosa.load(video_path, sr=16000)\n",
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "100%|██████████| 180/180 [02:36<00:00,  1.15it/s]\n",
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:122: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  converted_clips = torch.tensor(frames)\n"
     ]
    }
   ],
   "source": [
    "out = vp.extract_frames_and_audio(\"/DATA/sarmistha_2221cs21/basha/VideoMAE/dataset/_A5qfpLTbns.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data.pkl\",'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding frames\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid image shape. Expected either 4 or 3 dimensions, but got 1 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vision \u001b[38;5;241m=\u001b[39m \u001b[43mvp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/basha/VideoMAE/Design/video_processor.py:69\u001b[0m, in \u001b[0;36mVideoProcessor.vision_encoder\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m     67\u001b[0m encoded_frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_batch \u001b[38;5;129;01min\u001b[39;00m frames:\n\u001b[0;32m---> 69\u001b[0m     processed_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     72\u001b[0m     encoded_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprocessed_frames)\u001b[38;5;241m.\u001b[39mpooler_output\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m     73\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m     encoded_frames\u001b[38;5;241m.\u001b[39mappend(encoded_frame)\n",
      "File \u001b[0;32m~/anaconda3/envs/basha/lib/python3.10/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/basha/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:295\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m image_std \u001b[38;5;241m=\u001b[39m image_std \u001b[38;5;28;01mif\u001b[39;00m image_std \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_std\n\u001b[1;32m    293\u001b[0m do_convert_rgb \u001b[38;5;241m=\u001b[39m do_convert_rgb \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_convert_rgb\n\u001b[0;32m--> 295\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmake_list_of_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/basha/lib/python3.10/site-packages/transformers/image_utils.py:123\u001b[0m, in \u001b[0;36mmake_list_of_images\u001b[0;34m(images, expected_ndims)\u001b[0m\n\u001b[1;32m    121\u001b[0m         images \u001b[38;5;241m=\u001b[39m [images]\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image shape. Expected either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, but got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.ndarray, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid image shape. Expected either 4 or 3 dimensions, but got 1 dimensions."
     ]
    }
   ],
   "source": [
    "vision = vp.vision_encoder(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vp.image_model(**processor.image_processor([image,image], return_tensors=\"pt\").to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.pooler_output.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [49406, 932, 3337, 49407], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('basha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = clip_model.text_model(\n",
    "    input_ids = inputs['input_ids'],\n",
    "    attention_mask = inputs['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5152,  0.1658,  0.8876,  ..., -0.0675, -0.4551, -1.7960],\n",
       "        [ 0.0426,  0.0189,  1.2740,  ..., -0.4217, -0.4393, -1.3016]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.4560e-01, -5.7594e-01,  5.0856e-01,  1.7359e+00,  7.4192e-01,\n",
       "         9.6155e-01,  7.5606e-02,  5.1116e-01,  7.3742e-02,  2.1503e-01,\n",
       "         7.6160e-01, -1.4468e-01, -7.0118e-02,  1.0690e-01, -3.0795e-01,\n",
       "        -7.1974e-01,  5.3679e-01, -2.2584e-02, -4.2953e-01,  3.6920e-01,\n",
       "         3.8226e-01, -3.3620e-02, -1.6207e-01,  4.9019e-01, -7.5640e-02,\n",
       "        -9.1454e-03, -4.4394e-02, -2.5241e-01, -6.0639e-01,  1.0829e+00,\n",
       "         1.5226e-01,  3.7181e-02, -1.0843e+00, -3.0456e-01, -6.2940e-01,\n",
       "         3.8670e-02, -2.7472e-01, -6.9654e-01,  2.9603e-01,  2.3200e-01,\n",
       "         8.5503e-02,  2.6541e-01,  4.9576e-02,  4.4805e-01, -1.7676e-01,\n",
       "         7.1217e-01,  2.7446e-01, -3.2174e-01,  5.9483e-01,  9.7697e-02,\n",
       "         4.0697e-01,  5.5864e-01,  2.6820e+00,  5.6556e-01,  4.6075e-01,\n",
       "        -3.7861e-01,  4.9899e-01,  3.8797e-01,  1.6534e-01, -4.5438e-01,\n",
       "         4.3459e-01,  3.3482e-01, -6.9586e-02,  3.5913e-01, -6.5767e-01,\n",
       "         2.6411e-01,  9.3526e-01,  3.4101e-01, -1.1938e-01,  1.2431e-01,\n",
       "        -1.4387e-01,  3.8414e-01, -8.2192e-01, -1.0798e-01,  5.3588e-01,\n",
       "        -8.3560e-02,  3.1058e-01,  1.8512e-01,  7.2142e-01, -1.6537e-01,\n",
       "         6.4485e-02, -7.5720e-01,  2.2357e-01,  5.9830e-01, -8.7613e-01,\n",
       "         1.0090e+00,  2.0120e-01, -6.5831e-02, -5.9822e-03,  1.2086e+00,\n",
       "        -3.4744e-01,  9.4012e-01,  2.1771e-01,  5.2733e-01, -9.5805e-02,\n",
       "         1.7920e-01,  1.4705e+00,  6.2462e-02, -5.4672e-01, -2.4723e-01,\n",
       "        -3.1605e-01, -1.3093e-01, -3.2554e-01,  1.8484e-01,  7.1525e-01,\n",
       "         6.9135e-01, -8.8340e-02,  1.6225e-01, -2.1154e-01, -7.6422e-01,\n",
       "         5.3055e-01,  7.7916e-01, -2.0562e+00,  7.8006e-01,  4.6687e-01,\n",
       "         3.1253e-01,  2.9422e-01, -5.8360e-01,  5.9072e-01, -1.3747e-01,\n",
       "         1.8428e-01,  1.4675e-01,  4.4155e-01,  6.8293e-01,  3.7115e-01,\n",
       "         1.9208e-01, -3.2319e-01,  5.2283e-01,  1.0472e+00,  7.4659e-01,\n",
       "        -2.4744e-01,  2.4138e-01,  3.3963e-01,  4.8626e-01,  5.2335e-01,\n",
       "         6.5290e-01,  1.1687e+00,  1.6659e-01, -1.5742e-01,  4.5480e-01,\n",
       "         4.3985e-01, -2.9517e-01,  7.3066e-01, -5.9815e-01,  8.8998e-02,\n",
       "        -3.5852e-01, -9.3859e-02,  2.3715e-01, -9.7427e-01, -6.8240e-01,\n",
       "        -6.9666e-01,  4.7032e-01,  1.4402e-01,  7.5452e-01, -1.7041e+00,\n",
       "         3.6196e-01, -1.6698e-01,  2.1045e-01,  7.6591e-01,  5.8181e-01,\n",
       "         3.0092e-01, -2.5879e-01,  6.6880e-01, -6.4847e-01, -4.9698e-01,\n",
       "        -4.9181e-02, -5.6752e-01, -4.1060e-01,  5.4884e-01, -3.1892e-01,\n",
       "         5.0593e-01, -2.2417e-01,  3.8098e-01, -4.3899e-01,  5.5329e-01,\n",
       "        -9.4264e-01,  2.1003e-01,  1.5708e-01, -6.4605e-02, -9.8914e-01,\n",
       "        -6.0228e-01,  6.3663e-01, -3.9993e-01, -2.1650e-02, -4.5649e-01,\n",
       "         2.3973e-01,  5.2423e-01, -1.4984e-01, -4.3390e-01, -1.2079e-01,\n",
       "         7.1995e-01, -3.7827e-01,  9.6375e-01,  2.1194e-01,  1.6668e-02,\n",
       "         5.9061e-02, -9.5903e-01, -2.3964e+00,  4.4081e-01, -1.7854e-01,\n",
       "        -3.9592e-01,  9.4972e-01,  1.6439e-01,  2.9156e+00,  5.1451e-01,\n",
       "         4.9510e-01, -2.0539e-01,  2.4736e-01,  6.7212e-01,  6.2769e-01,\n",
       "         3.9696e-02, -6.7467e-01, -1.2432e+00,  9.2579e-01,  3.4289e-01,\n",
       "        -2.2879e-01,  8.3367e-01,  3.1645e-02,  2.5365e-01,  2.0857e-01,\n",
       "         7.4728e-01,  3.5841e-01, -8.4685e-01, -2.5283e-01,  5.8302e-01,\n",
       "         3.6171e-01, -7.6286e-02, -5.2555e-01,  4.1203e-01,  5.7789e-01,\n",
       "        -2.3817e-01,  7.5314e-01, -3.4059e-01, -1.7296e-01,  4.7558e-01,\n",
       "        -1.8444e-01,  5.0644e-01,  2.2550e-02,  8.1883e-01,  2.3272e-01,\n",
       "         7.0167e-04,  5.0029e-02,  2.2634e-01, -7.2752e-01,  3.2445e-01,\n",
       "        -4.7516e-01, -3.8780e-01, -7.0015e-01, -3.7224e-01,  2.4270e-02,\n",
       "         9.6031e-02, -1.7014e-01,  7.4704e-01,  9.6835e-03,  4.4010e-01,\n",
       "         8.8770e-01,  1.5751e-01,  3.3301e-01,  2.8631e-01,  7.1763e-03,\n",
       "        -6.5536e-01, -2.8716e-01,  4.2104e-01,  3.1207e-01, -3.1550e-01,\n",
       "        -2.4854e-01, -3.7853e-01,  4.5999e-01, -6.1532e-02,  1.9905e-01,\n",
       "         1.5010e-02,  6.1671e-01, -3.7986e-01,  1.2384e+00,  5.3390e-02,\n",
       "        -3.6520e-02,  2.7097e-01,  7.4196e-01,  6.5558e-01, -1.7316e-02,\n",
       "        -1.6951e-02,  4.2252e-01,  1.5811e-01, -2.0719e-01,  7.8066e-02,\n",
       "        -3.9023e-01, -3.6536e-01,  3.5547e-01, -9.9677e-02,  1.1115e+00,\n",
       "        -1.3271e+00, -5.3151e-01,  1.6529e-01,  1.2770e-01, -3.9550e-01,\n",
       "        -6.6471e-01, -1.6022e-01, -9.5848e-02,  5.6229e-01,  5.3728e-01,\n",
       "         2.4468e-01, -5.7719e-01,  1.3821e-02,  8.9056e-03,  1.0420e-01,\n",
       "         3.2999e-01, -8.5370e-01,  1.6811e-01,  1.6228e+00,  6.7571e-01,\n",
       "        -5.2484e-01,  2.0710e-01,  4.0606e-01, -1.2944e-01, -1.8843e+00,\n",
       "        -1.2222e-01,  3.7576e-01, -1.7374e-01,  1.4769e-01, -9.1557e-01,\n",
       "         1.4021e-01,  1.0130e-02,  2.5211e-01,  7.5631e-01,  1.0191e+00,\n",
       "        -3.6079e-01,  3.6208e-02, -5.8264e-01,  2.9575e-01,  8.1151e-02,\n",
       "        -5.6777e-01, -4.0144e-01, -4.5986e-01,  1.2425e+00, -1.1288e-01,\n",
       "        -6.0402e-01,  8.2702e-02, -3.3244e-01,  7.0091e-01,  6.3307e-01,\n",
       "        -1.2370e-01,  2.0402e-01,  5.0969e-01,  7.4631e-01,  5.9026e-01,\n",
       "         5.3083e-01,  8.8605e-01,  7.1602e-01, -9.6014e-01,  5.8774e-01,\n",
       "         8.8562e-01,  3.8931e-01,  8.1552e-01, -9.9937e-01, -6.4646e-02,\n",
       "        -6.2412e-01,  2.6731e-01, -8.1249e-01, -3.4439e-01, -5.2929e-01,\n",
       "        -2.5773e-01, -1.0344e-01, -5.9583e-01,  5.1968e-01,  7.6268e-01,\n",
       "         3.0696e-01, -4.3517e-01,  3.7790e-01,  1.0905e+00, -1.0598e+00,\n",
       "         3.0048e-01,  1.8376e-01, -2.1573e-01,  8.8164e-02, -8.1153e-01,\n",
       "         1.6725e-01,  5.0427e-01,  3.0856e-01,  5.5384e-01, -1.7020e-01,\n",
       "         2.5403e-01, -1.0037e-01,  3.1575e-01,  1.7256e+00, -9.8462e-01,\n",
       "         4.0307e-01,  1.9613e-01,  1.3875e-01,  1.2071e-01, -3.9205e-01,\n",
       "        -5.1544e-02, -7.6726e-02,  6.8317e-01,  3.4788e-01,  6.3689e-01,\n",
       "         7.1385e-01,  7.8925e-01,  5.1673e-02,  8.2653e-01,  4.8402e-02,\n",
       "         4.9318e-03,  4.6568e-03, -3.2519e-01,  8.9017e-01,  2.0687e-01,\n",
       "         3.4092e-01,  3.3040e-01,  6.9016e-01, -9.2748e-01,  2.7007e+00,\n",
       "         4.0365e-01, -4.8450e-01,  7.6313e-01,  7.8368e-01,  5.5085e-01,\n",
       "         1.0609e+00, -3.9789e-01, -2.3132e-01,  3.8926e-01, -6.3144e-01,\n",
       "         2.2749e-01, -1.1196e+00,  2.7438e-02,  1.5243e-01, -1.3516e+00,\n",
       "         2.5370e-02, -1.3096e+00, -3.7141e-01,  1.7294e-01, -4.6460e-01,\n",
       "        -2.6570e-01, -2.8255e-01, -1.1908e+00,  4.2748e-01,  3.8607e-01,\n",
       "         5.8013e-01,  1.7427e-01,  8.6925e-01,  3.9941e-01,  1.2195e+00,\n",
       "        -8.3883e-01,  1.4681e-01, -7.2074e-02,  2.7078e-01,  5.0362e-01,\n",
       "         1.2129e-01,  3.7753e-01,  4.1916e-01, -3.1852e-01, -2.7042e-01,\n",
       "        -3.0201e-01,  1.3985e-01,  2.0258e-01,  2.9995e-01,  1.0259e+00,\n",
       "         1.1759e-01, -4.4424e-01,  2.0773e-01, -2.2924e-01, -2.8050e-01,\n",
       "        -4.7072e-01, -3.2280e-02,  1.8603e-01,  4.0424e-02, -9.2828e-05,\n",
       "        -4.0167e-01,  6.7791e-01, -6.2318e-01, -1.1647e+00,  1.2938e+00,\n",
       "         7.2594e-04, -5.4789e-01,  4.5626e-01,  1.0282e+00,  5.8829e-01,\n",
       "         1.4120e+00, -6.5824e-01,  3.6383e-01, -6.9008e-02,  1.2438e+00,\n",
       "         4.6046e-01, -1.0260e-01,  7.8979e-01,  1.2353e+00,  8.3607e-01,\n",
       "         9.7429e-02,  2.5531e-01,  1.5183e-01,  4.1478e-01,  1.4278e-01,\n",
       "        -4.9631e-01,  7.7493e-01, -5.9690e-01,  4.4303e-01,  8.3075e-01,\n",
       "        -6.3525e-01,  5.7199e-01,  7.7226e-01,  6.1985e-01,  6.3465e-01,\n",
       "         5.1283e-01, -1.8869e-01, -5.6746e-01,  1.5416e-01, -1.7465e-01,\n",
       "         1.2150e-01,  7.9440e-02,  2.8048e-01, -4.6597e-01,  1.3073e-01,\n",
       "         3.2760e-01, -1.1023e+00], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
