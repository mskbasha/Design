{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"cat.jpg\")\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_processor import VideoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VideoProcessor(\n",
    "  clip_model.text_model,\n",
    "  clip_model.vision_model,\n",
    "  processor.tokenizer,\n",
    "  processor.image_processor,\n",
    "  clip_model.text_projection,\n",
    "  clip_model.visual_projection,\n",
    "  device = 'cpu'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = vp.to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vp.device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "video_dir = '/home/basha_2211ai03/complaint_detection/videos/dataset'\n",
    "videos = os.listdir(video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wNgOKJpk7K0.mp4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "try:\n",
    "    with open(\"data.pkl\",'rb') as f:\n",
    "        encoded_data = pickle.load(f)\n",
    "except:\n",
    "    encoded_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497753d66efc44389cc217b595dd1b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:208\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/home/basha_2211ai03/complaint_detection/videos/dataset/R6RHwjnibA0.mp4': Format not recognised.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m encoded_data:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m encoded_data[video] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/complaint_detection/Design/video_processor.py:55\u001b[0m, in \u001b[0;36mVideoProcessor.__call__\u001b[0;34m(self, video_loc)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_loc: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor:\n\u001b[0;32m---> 55\u001b[0m     frames, audios \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_frames_and_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_loc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     encoded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_encoder(frames)\n\u001b[1;32m     57\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_encoder(audios)\n",
      "File \u001b[0;32m~/complaint_detection/Design/video_processor.py:109\u001b[0m, in \u001b[0;36mVideoProcessor.extract_frames_and_audio\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m frame_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS))\n\u001b[1;32m    108\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT))\n\u001b[0;32m--> 109\u001b[0m audio_data, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    112\u001b[0m audio_clips \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:183\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/util/decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/librosa/core/audio.py:239\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    242\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/anaconda3/envs/complaint_detection/lib/python3.9/site-packages/audioread/__init__.py:132\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
      "\u001b[0;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "with torch.inference_mode():\n",
    "    for video in tqdm(videos):\n",
    "        if video in encoded_data:\n",
    "            continue\n",
    "        out = vp(os.path.join(video_dir,video))\n",
    "        encoded_data[video] = out\n",
    "        with open(\"data.pkl\",'wb') as f:\n",
    "            pickle.dump(encoded_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data.pkl\",'rb') as f:\n",
    "    out = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = vp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoProcessor(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (audio_model): Whisper(\n",
       "    (encoder): AudioEncoder(\n",
       "      (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x ResidualAttentionBlock(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TextDecoder(\n",
       "      (token_embedding): Embedding(51865, 512)\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x ResidualAttentionBlock(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn): MultiHeadAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (image_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:109: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, _ = librosa.load(video_path, sr=16000)\n",
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2 sec batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [02:24<00:00,  1.25it/s]\n",
      "/DATA/sarmistha_2221cs21/basha/VideoMAE/Design/video_processor.py:143: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  converted_clips = torch.tensor(frames)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2 sec batches complete\n",
      "Encoding frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:41<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding frames done\n",
      "Extracting text from audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180 [00:00<?, ?it/s]/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/whisper/transcribe.py:113: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/DATA/sarmistha_2221cs21/anaconda3/envs/basha/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 180/180 [05:49<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from audio completed\n",
      "Encoding Text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:10<00:00, 17.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vision = vp(\"/DATA/sarmistha_2221cs21/basha/VideoMAE/dataset/_A5qfpLTbns.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2062e-01,  4.5014e-02, -1.6820e-01, -3.4746e-02, -2.1113e-01,\n",
       "         -1.3153e-01,  1.2401e-01, -5.0429e-01, -1.8531e-02, -2.3232e-01,\n",
       "         -1.8971e-02, -3.7197e-02,  3.0465e-01,  4.0085e-01,  1.2778e-01,\n",
       "          1.1729e-01, -9.8357e-02, -9.7343e-02,  3.2400e-01, -3.3887e-01,\n",
       "          9.2911e-02, -2.6583e-01, -2.2583e-02, -7.6680e-02, -1.9461e-01,\n",
       "         -2.8250e-01,  1.1555e-01,  4.3266e-02, -5.6809e-02, -3.5991e-02,\n",
       "          4.3223e-02,  1.4839e-01,  1.7368e-01, -1.0038e-01, -1.2956e-01,\n",
       "          7.6926e-02, -2.2662e-01, -1.6489e-01,  1.6700e-01,  7.5899e-02,\n",
       "         -2.9313e-01, -2.8349e-01,  1.0109e-01,  8.1005e-03,  1.0431e-01,\n",
       "         -8.2173e-04,  1.4887e-01, -5.7052e-02, -7.0436e-03, -1.3565e-01,\n",
       "         -1.6587e-01,  3.3023e-01, -9.7329e-02,  1.3642e-01,  1.0315e-01,\n",
       "         -3.0204e-01, -1.8882e-02, -1.0632e-01,  3.2190e-01, -1.3384e-03,\n",
       "         -4.9162e-01, -3.6689e-01,  7.5371e-02,  4.1403e-03, -5.3402e-02,\n",
       "          9.5969e-02, -1.8406e-02,  4.9326e-01, -1.1642e-01, -7.6122e-02,\n",
       "         -1.4696e-01,  1.3152e-01, -4.2859e-01, -1.4945e-01, -2.2199e-01,\n",
       "         -2.4869e-01,  3.2821e-01,  1.0712e-01,  6.2700e-02, -1.0571e-01,\n",
       "          2.1466e-01, -9.5781e-02,  2.1642e-01,  4.2144e-01, -1.4273e-01,\n",
       "         -9.4595e-02,  1.4406e-01,  2.1464e-01,  4.7440e-01,  1.7754e-01,\n",
       "         -2.6204e-02,  5.8990e-02, -1.5311e+00, -5.5436e-02, -2.3580e-02,\n",
       "         -9.1653e-02, -2.2639e-01, -7.3488e-02, -4.2806e-01, -1.3058e-01,\n",
       "         -6.0936e-02, -1.2356e-01,  1.0361e-01,  1.8016e-01, -6.4200e-01,\n",
       "          2.0616e-01, -1.1322e-01, -1.5625e-01,  3.3891e-01,  3.2797e-02,\n",
       "         -3.4901e-01,  1.2080e+00, -1.8000e-01, -4.6432e-02, -1.8502e-01,\n",
       "          1.8249e-01,  3.4240e-02,  3.4221e-01, -6.4567e-02,  2.0944e-01,\n",
       "          3.6807e-02, -5.6738e-01,  9.9366e-02,  8.1468e-02,  1.7766e-01,\n",
       "          5.5506e-02,  2.9099e-02, -4.4288e-01, -2.7577e-02, -1.5497e-02,\n",
       "          2.8310e-01, -2.7419e-01,  1.7272e-01,  6.9542e+00, -5.9301e-02,\n",
       "         -1.9976e-01, -5.7875e-02,  3.6375e-01, -1.9124e-01, -1.3699e-01,\n",
       "         -2.7796e-01,  2.8606e-01,  9.1733e-02, -1.3766e-01,  8.4975e-02,\n",
       "         -1.2306e-01, -5.9397e-02, -1.7774e-01,  1.7979e-01, -5.9592e-01,\n",
       "          1.0019e-01, -6.0563e-02,  1.4635e-01,  1.4679e-01, -1.4380e-01,\n",
       "         -7.2905e-02,  7.4343e-02,  7.2663e-02, -5.0178e-02, -6.4885e-02,\n",
       "         -3.3593e-01,  2.0282e-01,  4.3937e-02, -2.8235e-03,  2.0563e-01,\n",
       "          4.9436e-02,  1.8013e-01, -4.0452e-01, -1.1557e-01,  8.1550e-02,\n",
       "         -2.4822e-01, -2.7796e-02, -1.1839e-01, -4.4283e-01,  3.7714e-01,\n",
       "          6.7154e-02, -4.1783e-01,  3.4957e-01,  1.6731e-01, -6.9683e-02,\n",
       "          1.0747e-01, -1.2307e-01,  9.0107e-04, -3.8167e-02,  2.0420e-01,\n",
       "          1.5939e-01, -3.9695e-02,  2.7748e-01, -2.5392e-01, -1.6328e-01,\n",
       "          3.1911e-01, -1.3006e-01, -2.2918e-01, -1.8566e-01,  1.4489e-01,\n",
       "          1.1283e-01, -2.2713e-01,  2.0417e-01,  2.0243e-01, -6.0990e-02,\n",
       "          2.7731e-02, -2.5567e-01,  1.5764e-01,  5.8029e-02,  7.6540e-02,\n",
       "          1.7570e-02, -2.7825e-02,  1.1061e-01, -2.9641e-02, -3.6532e-01,\n",
       "         -3.2033e-01, -1.0032e-01,  7.4519e-02,  3.3865e-01, -2.2703e-02,\n",
       "          1.0843e-01, -1.1913e-01,  9.6855e-02, -9.5795e-01,  3.2880e-02,\n",
       "         -1.6678e-01, -3.7853e-01, -1.6139e-01, -7.3266e-02, -2.2800e-01,\n",
       "          2.0200e-02, -1.6915e-01,  1.2691e-01,  2.7316e-01, -7.0726e-02,\n",
       "         -8.6809e-02, -6.2879e-02,  1.4526e-01, -7.1114e-02,  2.2127e-01,\n",
       "         -7.4600e-01,  1.6527e-01,  5.0748e-02, -1.2086e-01,  5.3782e-02,\n",
       "         -1.0509e-01, -1.5563e-01, -3.4978e-01,  4.6274e-02, -1.5276e-01,\n",
       "         -4.0213e-01,  9.5778e-02,  1.9698e-01,  5.1653e-02,  2.5275e-01,\n",
       "         -2.7577e-01, -1.9201e-01,  6.0601e-02,  9.5690e-02,  9.7773e-03,\n",
       "          4.7490e-02,  5.2865e-02, -3.8299e-03, -2.8724e-01, -1.4282e-01,\n",
       "         -3.6076e-01,  1.4987e-01,  6.4112e-02, -2.5446e-01, -6.5951e-02,\n",
       "         -7.2912e-02,  2.3107e-01,  1.4617e-01,  1.9920e-01, -6.0857e-02,\n",
       "          2.6100e-03, -2.2665e-01,  1.5437e-02, -8.6318e-02,  1.4716e-03,\n",
       "         -2.2034e-01, -1.5056e-01,  2.2503e-02, -1.8632e-01, -2.4931e-01,\n",
       "         -3.4723e-01,  2.8887e-01,  7.9560e-03, -1.4455e-03,  5.8414e-02,\n",
       "          4.5156e-01, -8.5148e-02, -1.9958e-01, -4.8804e-01,  3.2247e-02,\n",
       "          4.3469e-01, -4.3739e-02,  2.1662e-02,  1.0620e-01, -5.3439e-01,\n",
       "         -1.3055e-01, -2.2708e-01,  8.6595e-02, -3.7433e-02, -1.6968e-02,\n",
       "          7.8762e-02,  3.3803e-02,  1.2651e-01,  2.6457e-01,  9.2976e-02,\n",
       "          5.9110e-01,  1.4693e-01, -1.8926e-01,  5.4706e-02,  8.8293e-02,\n",
       "         -3.8535e-02, -1.9333e-01,  6.9510e+00,  5.0436e-02,  5.6331e-03,\n",
       "          9.2979e-02,  3.2365e-02,  4.8644e-01,  2.0073e-01,  1.2063e-01,\n",
       "          7.9998e-02,  2.6147e-01,  9.3003e-02, -5.2280e-02,  3.4084e-01,\n",
       "         -7.8207e-02,  4.7570e-02,  3.6292e-01,  1.1632e-01, -2.1962e+00,\n",
       "          2.5538e-01,  4.6117e-01,  2.8377e-01,  5.2169e-02, -1.4935e-02,\n",
       "          1.5080e-01,  1.3594e-01,  1.6760e-01,  3.0506e-01,  9.8981e-02,\n",
       "         -1.2392e-01, -6.3797e-02, -1.8362e-01,  1.5850e-01, -8.1656e-02,\n",
       "          6.2340e-02,  2.5344e-02,  3.0398e-01, -1.3185e-01,  3.2100e-02,\n",
       "          3.2883e-02,  3.0472e-01,  1.4305e-01,  3.1772e-01,  3.2137e-01,\n",
       "          6.0958e-01, -2.0709e-02,  9.2511e-02,  1.2298e-01, -5.1150e-01,\n",
       "         -3.7681e-01, -1.0083e-01,  1.1244e-01, -1.6162e-01, -2.3407e-01,\n",
       "         -8.3858e-02, -4.2550e-02,  4.2596e-01,  1.0608e-01, -7.3128e-02,\n",
       "          4.2857e-01,  4.4930e-01, -2.3494e-01, -1.7979e-01,  1.4967e-01,\n",
       "         -3.8301e-02, -2.2841e-01, -3.0592e-01, -1.9007e-01,  1.0432e-01,\n",
       "          4.2148e-01,  3.2286e-01, -2.9304e-01, -2.1553e-01, -1.4248e-01,\n",
       "          1.8091e-01,  9.7168e-02,  6.1880e-02,  5.7987e-02, -6.4734e-01,\n",
       "         -2.7251e-01,  2.5392e-01,  2.1647e-01, -2.3850e-01,  2.7617e-01,\n",
       "          3.5644e-01, -1.4517e-01,  3.2827e-02, -6.3215e-02,  3.8910e-01,\n",
       "          4.9750e-02, -4.1646e-01, -3.7541e-01,  3.6366e-01, -3.1982e-01,\n",
       "         -8.4147e-02,  2.1066e-01,  5.3055e-01,  2.7512e-01,  4.5504e-02,\n",
       "         -1.3629e-01,  1.7872e-01,  2.5893e-01,  1.8962e-02, -5.2401e-02,\n",
       "          1.8570e-01,  2.4235e-01,  2.5826e-02,  4.3108e-01,  3.8809e-01,\n",
       "         -1.1260e-01, -2.0355e-01, -3.9983e-02, -1.2299e-01, -2.9230e-01,\n",
       "         -8.8845e-03, -7.7554e-02,  4.6305e-01, -1.4372e-01, -1.6988e-01,\n",
       "          2.4974e-01,  2.2300e-01,  1.8829e-03,  9.0630e-02, -1.0492e-01,\n",
       "         -3.0206e-01,  5.3917e-02,  2.0903e-01, -2.2727e-01, -4.1900e-01,\n",
       "         -2.5885e-01,  6.0922e-02, -9.1705e-02,  3.2239e-01, -1.4309e-01,\n",
       "          1.0093e-01,  6.2606e-02, -2.3036e-01,  1.3927e-01, -6.4014e-02,\n",
       "          2.9734e-01,  1.1905e-01,  8.6867e-03,  8.5122e-02,  8.4519e-03,\n",
       "         -1.2967e-01,  3.2329e-01, -1.8781e-01,  2.1230e-01,  4.8638e-01,\n",
       "          1.7768e-01,  1.5441e-01, -2.7755e-01,  6.4519e-02, -1.9959e-01,\n",
       "          2.1285e-01, -2.4689e-01,  5.5606e-02, -1.6996e-01, -9.8415e-02,\n",
       "          9.9920e-02, -7.4238e-02, -2.0113e-02,  1.3819e-01, -4.0868e-02,\n",
       "          4.5768e-01, -2.3049e-02,  6.3881e-02,  1.5413e-01,  1.5993e-01,\n",
       "          3.6332e-01, -1.5200e-01, -5.3066e-02, -1.8015e-01,  1.2580e-02,\n",
       "         -1.8310e-01, -1.2623e-01, -2.5761e-02, -7.3246e-02, -4.2941e-02,\n",
       "          1.1606e-01, -1.1271e-01,  7.2115e-02, -7.0163e-01, -4.4164e-02,\n",
       "          1.5147e-01,  1.9115e-01,  3.4653e-01, -9.2961e-02, -9.8659e-03,\n",
       "          4.0687e-01,  2.7851e-01, -2.2720e-01, -3.1043e-02,  1.2847e-01,\n",
       "         -1.2166e-01, -2.1689e-02,  1.1084e-01, -1.4049e-01,  1.6401e-01,\n",
       "         -2.5425e-01,  2.5947e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model.text_projection(vision[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vision[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vp.image_model(**processor.image_processor([image,image], return_tensors=\"pt\").to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.pooler_output.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer('basha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = clip_model.text_model(\n",
    "    input_ids = inputs['input_ids'],\n",
    "    attention_mask = inputs['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.last_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complaint_detection",
   "language": "python",
   "name": "complaint_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
