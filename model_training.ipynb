{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DetectSpan\n",
    "import pandas as pd\n",
    "from utils import pad, filter_and_pad_outputs\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(x):\n",
    "    y = 1 \n",
    "    for i in x:\n",
    "        y *= i\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"final_complaints.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '0.25.2.30' 2 0.25:2.30\n",
      "could not convert string to float: 'o 0.44' 0 0:21 too 0:44\n",
      "could not convert string to float: '' 0 0:09 to \n"
     ]
    }
   ],
   "source": [
    "def convert(x):\n",
    "    try:\n",
    "        x = [\n",
    "            float(\n",
    "                y.\\\n",
    "                strip(' ').\\\n",
    "                replace(':','.').\\\n",
    "                replace(';','.').\\\n",
    "                replace('-','.')\n",
    "            ) \n",
    "            for y in x.strip(':. ').split('to' if 'to' in x else '-')\n",
    "        ] if type(x)==str else x\n",
    "    except Exception as e:\n",
    "        print(e,x.count('.'),x)\n",
    "        x = None\n",
    "    return x\n",
    "df['Timing_1'] = df['Timing_1'].apply(convert)\n",
    "df['Timing_2'] = df['Timing_2'].apply(convert)\n",
    "df['Timing_3'] = df['Timing_3'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = ['Timing_1','Timing_2','Timing_3']\n",
    "for col in time_cols:\n",
    "    df[col] = df[col].apply(\n",
    "        lambda x:\n",
    "            [\n",
    "                int(x[0])*60+( x[0]-int(x[0]) )*60,\n",
    "                int(x[1])*60+( x[1]-int(x[1]) )*60\n",
    "            ]\n",
    "            if type(x) is list and x and x is not np.nan else [0,0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data.pkl\",'rb') as f:\n",
    "#     dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = []\n",
    "\n",
    "# for ind,( key, value) in enumerate(dataset.items()):\n",
    "#     print(f\"\\r {ind}/ {len(dataset)}\",end =  ' ')\n",
    "#     total_time = value[0].shape[0] \n",
    "#     start = 0\n",
    "#     labels = []\n",
    "#     while start < total_time:\n",
    "#         label_found = False\n",
    "#         try:\n",
    "#             for timings in df[df[\"Video ID\"] == key[:-4]][time_cols].iloc[0].values:\n",
    "#                 if timings and timings is not np.nan and type(timings) is list and curr_time in range(int(timings[0]), int(timings[1])):\n",
    "#                     labels.append(1)\n",
    "#                     label_found = True\n",
    "#         except:\n",
    "#             pass\n",
    "#         if not label_found:\n",
    "#             labels.append(0)\n",
    "#         start += 1\n",
    "#     new_dataset.append([[x.cpu() for x in value], labels,df[df[\"Video ID\"] == key[:-4]][time_cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_with_labels.pkl','rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detector = DetectSpan(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detector = span_detector.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(span_detector.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_detector.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.0897810459136963 \n",
      "Loss = 0.5543214082717896 \n",
      "Loss = 0.5519925355911255 \n",
      "Loss = 0.551717221736908 \n",
      "Loss = 0.5516787171363831 \n",
      "Loss = 0.5516864061355591 \n",
      "Loss = 0.5516592264175415 \n",
      "Loss = 0.5516239404678345 \n",
      "Loss = 0.5516027808189392 \n",
      "Loss = 0.5515934228897095 \n",
      "Loss = 0.5515871047973633 \n",
      "Loss = 0.5515790581703186 \n",
      "Loss = 0.5515691041946411 \n",
      "Loss = 0.5515577793121338 \n",
      "Loss = 0.5515464544296265 \n",
      "Loss = 0.5515367388725281 \n",
      "Loss = 0.5515297651290894 \n",
      "Loss = 0.5515246391296387 \n",
      "Loss = 0.5515213012695312 \n",
      "Loss = 0.551519513130188 \n",
      "Loss = 0.551517128944397 \n",
      "Loss = 0.5515145659446716 \n",
      "Loss = 0.5515120029449463 \n",
      "Loss = 0.551509439945221 \n",
      "Loss = 0.551506757736206 \n",
      "Loss = 0.5515050292015076 \n",
      "Loss = 0.5515037178993225 \n",
      "Loss = 0.5515026450157166 \n",
      "Loss = 0.5515013337135315 \n",
      "Loss = 0.5515000224113464 \n",
      "Loss = 0.5514986515045166 \n",
      "Loss = 0.5514973998069763 \n",
      "Loss = 0.5514960885047913 \n",
      "Loss = 0.5514950752258301 \n",
      "Loss = 0.5514942407608032 \n",
      "Loss = 0.5514934659004211 \n",
      "Loss = 0.5514926910400391 \n",
      "Loss = 0.5514925718307495 \n",
      "Loss = 0.5514920949935913 \n",
      "Loss = 0.5514915585517883 \n",
      "Loss = 0.5514912605285645 \n",
      "Loss = 0.5514909029006958 \n",
      "Loss = 0.551490306854248 \n",
      "Loss = 0.5514898300170898 \n",
      "Loss = 0.5514891743659973 \n",
      "Loss = 0.5514889359474182 \n",
      "Loss = 0.55148845911026 \n",
      "Loss = 0.5514882206916809 \n",
      "Loss = 0.5514878034591675 \n",
      "Loss = 0.551487147808075 \n",
      "Loss = 0.5514870285987854 \n",
      "Loss = 0.551486611366272 \n",
      "Loss = 0.5514863133430481 \n",
      "Loss = 0.5514858365058899 \n",
      "Loss = 0.551485538482666 \n",
      "Loss = 0.5514852404594421 \n",
      "Loss = 0.5514846444129944 \n",
      "Loss = 0.5514845252037048 \n",
      "Loss = 0.551484227180481 \n",
      "Loss = 0.5514839291572571 \n",
      "Loss = 0.5514835715293884 \n",
      "Loss = 0.5514833331108093 \n",
      "Loss = 0.5514830946922302 \n",
      "Loss = 0.5514828562736511 \n",
      "Loss = 0.5514823794364929 \n",
      "Loss = 0.551482081413269 \n",
      "Loss = 0.5514816045761108 \n",
      "Loss = 0.5514814257621765 \n",
      "Loss = 0.5514810681343079 \n",
      "Loss = 0.5514809489250183 \n",
      "Loss = 0.5514805912971497 \n",
      "Loss = 0.551480233669281 \n",
      "Loss = 0.5514799356460571 \n",
      "Loss = 0.5514795184135437 \n",
      "Loss = 0.5514793395996094 \n",
      "Loss = 0.5514791011810303 \n",
      "Loss = 0.5514786839485168 \n",
      "Loss = 0.5514784455299377 \n",
      "Loss = 0.5514779686927795 \n",
      "Loss = 0.5514777898788452 \n",
      "Loss = 0.5514774322509766 \n",
      "Loss = 0.5514772534370422 \n",
      "Loss = 0.5514768958091736 \n",
      "Loss = 0.5514766573905945 \n",
      "Loss = 0.551476240158081 \n",
      "Loss = 0.5514760613441467 \n",
      "Loss = 0.5514757633209229 \n",
      "Loss = 0.5514754056930542 \n",
      "Loss = 0.5514751076698303 \n",
      "Loss = 0.5514748096466064 \n",
      "Loss = 0.5514744520187378 \n",
      "Loss = 0.5514741539955139 \n",
      "Loss = 0.5514737367630005 \n",
      "Loss = 0.5514735579490662 \n",
      "Loss = 0.5514731407165527 \n",
      "Loss = 0.5514729619026184 \n",
      "Loss = 0.551472544670105 \n",
      "Loss = 0.5514721870422363 \n",
      "Loss = 0.5514720678329468 \n",
      "Loss = 0.5514715909957886 \n",
      "Loss = 0.5514712333679199 \n",
      "Loss = 0.5514709949493408 \n",
      "Loss = 0.5514706969261169 \n",
      "Loss = 0.5514706373214722 \n",
      "Loss = 0.5514702200889587 \n",
      "Loss = 0.5514698028564453 \n",
      "Loss = 0.5514695048332214 \n",
      "Loss = 0.5514692664146423 \n",
      "Loss = 0.5514690279960632 \n",
      "Loss = 0.5514686703681946 \n",
      "Loss = 0.5514686107635498 \n",
      "Loss = 0.551468014717102 \n",
      "Loss = 0.5514678955078125 \n",
      "Loss = 0.5514677166938782 \n",
      "Loss = 0.5514672994613647 \n",
      "Loss = 0.5514671802520752 \n",
      "Loss = 0.551466703414917 \n",
      "Loss = 0.5514665246009827 \n",
      "Loss = 0.5514661073684692 \n",
      "Loss = 0.5514658689498901 \n",
      "Loss = 0.5514655113220215 \n",
      "Loss = 0.5514652132987976 \n",
      "Loss = 0.5514650344848633 \n",
      "Loss = 0.5514646768569946 \n",
      "Loss = 0.5514645576477051 \n",
      "Loss = 0.5514641404151917 \n",
      "Loss = 0.551463782787323 \n",
      "Loss = 0.5514636635780334 \n",
      "Loss = 0.5514631867408752 \n",
      "Loss = 0.5514630675315857 \n",
      "Loss = 0.5514628291130066 \n",
      "Loss = 0.5514626502990723 \n",
      "Loss = 0.5514624714851379 \n",
      "Loss = 0.5514622926712036 \n",
      "Loss = 0.5514618754386902 \n",
      "Loss = 0.551461935043335 \n",
      "Loss = 0.551461398601532 \n",
      "Loss = 0.5514612197875977 \n",
      "Loss = 0.5514611601829529 \n",
      "Loss = 0.5514609813690186 \n",
      "Loss = 0.5514605641365051 \n",
      "Loss = 0.5514603853225708 \n",
      "Loss = 0.5514605045318604 \n",
      "Loss = 0.5514600276947021 \n",
      "Loss = 0.5514601469039917 \n",
      "Loss = 0.5514594912528992 \n",
      "Loss = 0.551459550857544 \n",
      "Loss = 0.5514590740203857 \n",
      "Loss = 0.5514591932296753 \n",
      "Loss = 0.5514587759971619 \n",
      "Loss = 0.5514586567878723 \n",
      "Loss = 0.551458477973938 \n",
      "Loss = 0.5514583587646484 \n",
      "Loss = 0.5514581203460693 \n",
      "Loss = 0.5514580011367798 \n",
      "Loss = 0.5514578223228455 \n",
      "Loss = 0.5514575839042664 \n",
      "Loss = 0.5514573454856873 \n",
      "Loss = 0.5514570474624634 \n",
      "Loss = 0.551456868648529 \n",
      "Loss = 0.5514570474624634 \n",
      "Loss = 0.55145663022995 \n",
      "Loss = 0.5514567494392395 \n",
      "Loss = 0.5514561533927917 \n",
      "Loss = 0.5514561533927917 \n",
      "Loss = 0.5514559745788574 \n",
      "Loss = 0.5514557957649231 \n",
      "Loss = 0.5514558553695679 \n",
      "Loss = 0.5514552593231201 \n",
      "Loss = 0.5514556765556335 \n",
      "Loss = 0.551455557346344 \n",
      "Loss = 0.5514549612998962 \n",
      "Loss = 0.5514550805091858 \n",
      "Loss = 0.5514546632766724 \n",
      "Loss = 0.5514549016952515 \n",
      "Loss = 0.551454484462738 \n",
      "Loss = 0.5514546036720276 \n",
      "Loss = 0.5514540672302246 \n",
      "Loss = 0.5514542460441589 \n",
      "Loss = 0.5514541268348694 \n",
      "Loss = 0.5514540672302246 \n",
      "Loss = 0.5514540076255798 \n",
      "Loss = 0.5514540076255798 \n",
      "Loss = 0.5514538288116455 \n",
      "Loss = 0.551453709602356 \n",
      "Loss = 0.5514535307884216 \n",
      "Loss = 0.5514535307884216 \n",
      "Loss = 0.5514535307884216 \n",
      "Loss = 0.5514534115791321 \n",
      "Loss = 0.5514533519744873 \n",
      "Loss = 0.551453173160553 \n",
      "Loss = 0.5514532327651978 \n",
      "Loss = 0.5514532923698425 \n",
      "Loss = 0.5514531135559082 \n",
      "Loss = 0.5514529943466187 \n",
      "Loss = 0.5514530539512634 \n",
      "Loss = 0.5514526963233948 \n",
      "Loss = 0.5514529347419739 \n",
      "Loss = 0.55145263671875 \n",
      "Loss = 0.5514525175094604 \n",
      "Loss = 0.5514527559280396 \n",
      "Loss = 0.5514522790908813 \n",
      "Loss = 0.5514522790908813 \n",
      "Loss = 0.551452100276947 \n",
      "Loss = 0.5514521598815918 \n",
      "Loss = 0.5514519810676575 \n",
      "Loss = 0.5514522790908813 \n",
      "Loss = 0.5514517426490784 \n",
      "Loss = 0.5514516830444336 \n",
      "Loss = 0.551451563835144 \n",
      "Loss = 0.5514513850212097 \n",
      "Loss = 0.5514512658119202 \n",
      "Loss = 0.5514513254165649 \n",
      "Loss = 0.5514512658119202 \n",
      "Loss = 0.5514509081840515 \n",
      "Loss = 0.551451563835144 \n",
      "Loss = 0.5514511466026306 \n",
      "Loss = 0.5514514446258545 \n",
      "Loss = 0.5514507293701172 \n",
      "Loss = 0.5514510273933411 \n",
      "Loss = 0.5514506101608276 \n",
      "Loss = 0.5514512062072754 \n",
      "Loss = 0.5514511466026306 \n",
      "Loss = 0.5514510869979858 \n",
      "Loss = 0.5514505505561829 \n",
      "Loss = 0.5514501333236694 \n",
      "Loss = 0.5514510273933411 \n",
      "Loss = 0.5514498353004456 \n",
      "Loss = 0.5514498949050903 \n",
      "Loss = 0.5514510273933411 \n",
      "Loss = 0.5514499545097351 \n",
      "Loss = 0.5514505505561829 \n",
      "Loss = 0.5514498353004456 \n",
      "Loss = 0.5514503717422485 \n",
      "Loss = 0.5514495372772217 \n",
      "Loss = 0.5514501929283142 \n",
      "Loss = 0.5514495968818665 \n",
      "Loss = 0.5514495372772217 \n",
      "Loss = 0.5514495968818665 \n",
      "Loss = 0.5514494180679321 \n",
      "Loss = 0.5514495968818665 \n",
      "Loss = 0.5514494776725769 \n",
      "Loss = 0.5514492392539978 \n",
      "Loss = 0.5514492988586426 \n",
      "Loss = 0.551449179649353 \n",
      "Loss = 0.5514491200447083 \n",
      "Loss = 0.5514489412307739 \n",
      "Loss = 0.5514494180679321 \n",
      "Loss = 0.5514490604400635 \n",
      "Loss = 0.5514492988586426 \n",
      "Loss = 0.5514485836029053 \n",
      "Loss = 0.5514485239982605 \n",
      "Loss = 0.55144864320755 \n",
      "Loss = 0.5514487624168396 \n",
      "Loss = 0.5514490604400635 \n",
      "Loss = 0.5514482855796814 \n",
      "Loss = 0.5514490604400635 \n",
      "Loss = 0.5514489412307739 \n",
      "Loss = 0.5514481067657471 \n",
      "Loss = 0.5514488816261292 \n",
      "Loss = 0.5514480471611023 \n",
      "Loss = 0.5514489412307739 \n",
      "Loss = 0.5514481067657471 \n",
      "Loss = 0.55144864320755 \n",
      "Loss = 0.5514479875564575 \n",
      "Loss = 0.5514487624168396 \n",
      "Loss = 0.5514484643936157 \n",
      "Loss = 0.55144864320755 \n",
      "Loss = 0.5514488220214844 \n",
      "Loss = 0.551448404788971 \n",
      "Loss = 0.5514484643936157 \n",
      "Loss = 0.5514482259750366 \n",
      "Loss = 0.5514485239982605 \n",
      "Loss = 0.5514475703239441 \n",
      "Loss = 0.5514478087425232 \n",
      "Loss = 0.5514475107192993 \n",
      "Loss = 0.5514482259750366 \n",
      "Loss = 0.551447868347168 \n",
      "Loss = 0.5514482259750366 \n",
      "Loss = 0.5514473915100098 \n",
      "Loss = 0.5514474511146545 \n",
      "Loss = 0.5514476895332336 \n",
      "Loss = 0.5514479875564575 \n",
      "Loss = 0.5514483451843262 \n",
      "Loss = 0.5514472723007202 \n",
      "Loss = 0.5514481067657471 \n",
      "Loss = 0.5514481663703918 \n",
      "Loss = 0.5514470934867859 \n",
      "Loss = 0.5514481067657471 \n",
      "Loss = 0.5514475107192993 \n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "epochs = 10\n",
    "loss_cache = []\n",
    "for epoch in range(epochs):\n",
    "    for index in range(0, len(dataset), batch_size):\n",
    "        video_features = [x[0][0] for x in dataset[index : index + batch_size]]\n",
    "        audio_features = [x[0][1].squeeze(1) for x in dataset[index : index + batch_size]]\n",
    "        outputs = [x[1] for x in dataset[index : index + batch_size]]\n",
    "        video_features, mask = pad(video_features)\n",
    "        audio_features, mask = pad(audio_features)\n",
    "        features = video_features + audio_features\n",
    "        outputs = filter_and_pad_outputs(outputs)\n",
    "        pred = span_detector(features.cuda(), attention_mask=mask.cuda())\n",
    "        l = loss(pred.view(-1, 3), outputs.view(-1).cuda())\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"\\rLoss = {l.item()} \")\n",
    "        loss_cache.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pred,axis =-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
